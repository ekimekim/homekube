network map:
	hosts will be on the home 192.168.42.0/24 as normal
	services: 192.168.43.0/24
	pods: 192.168.64.0/19
		with each node having a /24 within that

DNS structure:
	ekime.kim: Externally managed (namecheap), points to my public IP
	*.ekime.kim: For public-facing services, CNAMEd to ekime.kim
	*.xenon.ekime.kim: For internal services, CNAMEd to internal ingress controller service
		TODO: This should point somewhere that non-service-ip aware machines can reach it, a node port.
	NAME.NAMESPACE.svc.xenon.ekime.kim: Service DNS
	**.xenon.ekime.kim is CNAMEd externally (overridden internall) to ekime.kim for cert-issuance purposes only.

File structure:
	ca/ - certificate management, CSRs, certs and keys
		nodes/ - certs for specific nodes
	generated-files - cluster management generated yaml files, eg. configs
	files/ - cluster management non-generated files, eg. configs
	kubeconfigs/ - kubeconfig files (ie. auth) for cluster components
		nodes/ - kubeconfigs for specific node kubelets
	manifests/ - api objects to deploy to the cluster
		*.libsonnet - Shared libraries
		*.jsonnet - Un-namespaced resources
		NAMESPACE/*.jsonnet - Namespaced resources
	secrets/ - files generated by scripts, generally random keys, etc.
	static-pods/ - Pod objects that are not deployed via apiserver but run as static kubelet pods

deferred for now: API server should be HA
	this requires a HA IP (eg. with keepalived)
	for now will just hard-code charm for critical components (etcd, api server)

TODO:
	prometheus
		prometheus can't scrape controller-manager or scheduler (403)
		scrape itself
	grafana
		grafana should ideally reach a point it has no state
			auth - is there a better option than hard-coded password? probably not.
	alertmanager
	logging
