network map:
	hosts will be on the home 192.168.42.0/24 as normal
	services: 192.168.43.0/24
	pods: 192.168.64.0/19
		with each node having a /24 within that
the tutorials i'm following are using this network map:
	hosts: 10.240.0.0/24
		with masters on 10.240.0.{10,11,12}
		and workers on 10.240.0.{20,21,22} (with pod ranges 10.200.{0,1,2}.0/24 respectively)
	pods: 10.200.0.0/16
	services: 10.32.0.0/24

File structure:
	ca/ - certificate management, CSRs, certs and keys
		nodes/ - certs for specific nodes
	generated-files - general generated yaml files, eg. configs
	files/ - general non-generated files, eg. configs
	kubeconfigs/ - kubeconfig files
		nodes/ - kubeconfigs for specific nodes
	manifests/ - api objects to deploy to the cluster
	secrets/ - files generated by scripts, generally random keys, etc.
	static-pods/ - Pod objects that are not deployed via apiserver but run as static kubelet pods

deferred for now: API server should be HA
	this requires a HA IP (eg. with keepalived)
	for now will just hard-code charm for critical components (etcd, api server)

next problem: pods are getting 192.168.43.0/24 ips, even though that's for services.
	This is due to containerd's CNI configuration under /etc/cni/net.d/10-k8s.conflist
		{
			"cniVersion": "1.0.0",
			"name": "k8s",
			"plugins": [
				{
					"type": "bridge",
					"bridge": "k8s",
					"isDefaultGateway": true,
					"ipMasq": true,
					"hairpinMode": true,
					"ipam": {
						"type": "host-local",
						"subnet": "192.168.43.0/24"
					}
				},
				{
					"type": "portmap",
					"capabilities": {"portMappings": true}
				}
			]
		}
	This is required node-level setup that I didn't talk about before(?)

DNS
	I've set up coredns, but it relies on the kubernetes api service which uses a service IP
		This in turn means we need the service ip on the api-server certificate
	An interesting question is how to set up pods to use it. We can't reference it by service name
	because it's the thing that resolves service names. We have to use a hard-coded cluster IP.
	Easiest way to do that is to pick one a-priori and state it explicitly during Service creation.
		How do we prevent/handle this IP already being in use?
	Also still TODO, service account and cluster role for coredns. I think it just needs read-only EndpointSlices?

kube-proxy
	We have another bootstrap issue here. kube-proxy needs to talk to the api server
	but the api server is only exposed as a service ip, which requires kube-proxy.
	We also can't use a headless service as that requires DNS and coredns would then have its own
	bootstrap issue.
	kubeadm etc solve this by having some external load balancer on the apiserver ip.
	I should treat this the same way I treat other components like controller-manager and have it
	connect to the apiserver node ip directly.

	In theory I should be able to use a serviceAccount when connecting, instead of having long-lived
	kubeconfig credentials like other components do. TODO delete those kube-proxy kubeconfigs.

	The service account isn't working for unknown reasons. Tried letting it pick up the token
	automatically, tried setting it explicitly via kubeconfig.
	kubeadm has a way of making it work, but I don't have an example to work from.
